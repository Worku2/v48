---
title: Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters
abstract: Hyperparameter selection generally relies on running multiple full training
  trials, with selection based on validation set performance. We propose a gradient-based
  approach for locally adjusting hyperparameters during training of the model. Hyperparameters
  are adjusted so as to make the model parameter gradients, and hence updates, more
  advantageous for the validation cost. We explore the approach for tuning regularization
  hyperparameters and find that in experiments on MNIST, SVHN and CIFAR-10, the resulting
  regularization levels are within the optimal regions. The additional computational
  cost depends on how frequently the hyperparameters are trained, but the tested scheme
  adds only 30% computational overhead regardless of the model size. Since the method
  is significantly less computationally demanding compared to similar gradient-based
  approaches to hyperparameter optimization, and consistently finds good hyperparameter
  values, it can be a useful tool for training neural network models.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: luketina16
month: 0
tex_title: Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters
firstpage: 2952
lastpage: 2960
page: 2952-2960
order: 2952
cycles: false
author:
- given: Jelena
  family: Luketina
- given: Mathias
  family: Berglund
- given: Klaus
  family: Greff
- given: Tapani
  family: Raiko
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/luketina16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
